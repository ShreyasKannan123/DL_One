{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnphA-iX5_8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the ZIP file\n",
        "zip_file_path = 'fma_img.zip'\n",
        "\n",
        "# Define the path to the folder where you want to extract the files\n",
        "extract_folder_path = 'images'\n",
        "\n",
        "try:\n",
        "    # Create the extract folder if it doesn't exist\n",
        "    if not os.path.exists(extract_folder_path):\n",
        "        os.makedirs(extract_folder_path)\n",
        "        print(f\"Folder {extract_folder_path} created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder {extract_folder_path} already exists.\")\n",
        "\n",
        "    # Extract the ZIP file to the extract folder\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder_path)\n",
        "    print(f\"Files extracted to {extract_folder_path} successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {zip_file_path} was not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Error: You don't have permission to create the folder {extract_folder_path} or extract the ZIP file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWItk3RF6A71",
        "outputId": "bed725bc-4f9c-452a-8b84-39f8be62d2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder images already exists.\n",
            "Files extracted to images successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFac4vfa5mIi",
        "outputId": "d6da54ee-60ff-4d71-9f6f-f7883e3b6bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 230 images belonging to 10 classes.\n",
            "Found 50 images belonging to 10 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "15/15 [==============================] - 18s 934ms/step - loss: 2.4844 - accuracy: 0.0783 - val_loss: 2.4214 - val_accuracy: 0.1000\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 10s 665ms/step - loss: 2.4566 - accuracy: 0.0565 - val_loss: 2.3738 - val_accuracy: 0.1000\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 12s 758ms/step - loss: 2.4173 - accuracy: 0.1043 - val_loss: 2.3206 - val_accuracy: 0.1000\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 2.4016 - accuracy: 0.0739 - val_loss: 2.3350 - val_accuracy: 0.1000\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 11s 716ms/step - loss: 2.4019 - accuracy: 0.0913 - val_loss: 2.3090 - val_accuracy: 0.1000\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 10s 669ms/step - loss: 2.3594 - accuracy: 0.0913 - val_loss: 2.3209 - val_accuracy: 0.2000\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 2.3856 - accuracy: 0.0826 - val_loss: 2.3174 - val_accuracy: 0.1000\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.3790 - accuracy: 0.0826 - val_loss: 2.3116 - val_accuracy: 0.1000\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 14s 929ms/step - loss: 2.3737 - accuracy: 0.1087 - val_loss: 2.3383 - val_accuracy: 0.1000\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 14s 947ms/step - loss: 2.3752 - accuracy: 0.0913 - val_loss: 2.3104 - val_accuracy: 0.1000\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 15s 1s/step - loss: 2.3846 - accuracy: 0.1000 - val_loss: 2.3442 - val_accuracy: 0.1200\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 12s 825ms/step - loss: 2.4031 - accuracy: 0.0739 - val_loss: 2.3314 - val_accuracy: 0.1000\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 12s 825ms/step - loss: 2.3573 - accuracy: 0.1087 - val_loss: 2.3274 - val_accuracy: 0.1000\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 13s 918ms/step - loss: 2.3910 - accuracy: 0.0783 - val_loss: 2.3419 - val_accuracy: 0.1000\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 2.3882 - accuracy: 0.0913 - val_loss: 2.3110 - val_accuracy: 0.1000\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 11s 765ms/step - loss: 2.3491 - accuracy: 0.0739 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 11s 721ms/step - loss: 2.3431 - accuracy: 0.0957 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 2.3388 - accuracy: 0.1130 - val_loss: 2.3348 - val_accuracy: 0.1000\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 11s 763ms/step - loss: 2.3762 - accuracy: 0.0957 - val_loss: 2.3172 - val_accuracy: 0.1000\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 2.3754 - accuracy: 0.0870 - val_loss: 2.3166 - val_accuracy: 0.1000\n",
            "Training Accuracy: 0.08695652335882187\n",
            "Validation Accuracy: 0.10000000149011612\n",
            "Training Loss: 2.3754098415374756\n",
            "Validation Loss: 2.316619634628296\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from PIL import Image, ImageTk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the path to the main directory containing subdirectories with images\n",
        "main_dir = r'/content/extracted_files/fma_img'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "batch_size = 16\n",
        "\n",
        "# Define the data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    main_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use subset parameter for training data\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    main_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use subset parameter for validation data\n",
        ")\n",
        "\n",
        "# Load the ResNet-50 model without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Freeze the convolutional base\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a classification head\n",
        "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
        "x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
        "\n",
        "# Compile the model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=20, validation_data=validation_generator)\n",
        "\n",
        "# Print accuracy and other performance metrics\n",
        "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
        "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n",
        "print(\"Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Validation Loss:\", history.history['val_loss'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from PIL import Image, ImageTk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "# Set the path to the main directory containing subdirectories with images\n",
        "main_dir = r'/content/extracted_files/fma_img'\n",
        "\n",
        "# Define image dimensions and batch size (EfficientNetB7 uses 600x600 input size)\n",
        "img_height = 100\n",
        "img_width = 100\n",
        "batch_size = 8  # Adjusted for the larger image size and complexity\n",
        "\n",
        "# Define the data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    main_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use subset parameter for training data\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    main_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use subset parameter for validation data\n",
        ")\n",
        "\n",
        "# Load the EfficientNetB7 model without the top classification layer\n",
        "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Freeze the convolutional base\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a classification head\n",
        "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
        "x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
        "\n",
        "# Compile the model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=50, validation_data=validation_generator)\n",
        "\n",
        "# Print accuracy and other performance metrics\n",
        "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
        "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n",
        "print(\"Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Validation Loss:\", history.history['val_loss'][-1])\n",
        "\n",
        "# Function to preprocess the uploaded image\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_height, img_width))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Function to calculate embeddings\n",
        "def calculate_embeddings(image_path):\n",
        "    img_array = preprocess_image(image_path)\n",
        "    embeddings = model.predict(img_array)\n",
        "    return embeddings\n",
        "\n",
        "# Function to open file dialog and get the image path\n",
        "def browse_file():\n",
        "    file_path = filedialog.askopenfilename()\n",
        "    image_path_var.set(file_path)\n",
        "    if file_path:\n",
        "        img = Image.open(file_path)\n",
        "        img = img.resize((200, 200), Image.ANTIALIAS)\n",
        "        img = ImageTk.PhotoImage(img)\n",
        "        image_label.config(image=img)\n",
        "        image_label.image = img\n",
        "\n",
        "# Function to match embeddings and determine class\n",
        "def match_embeddings():\n",
        "    image_path = image_path_var.get()\n",
        "    if image_path:\n",
        "        uploaded_embeddings = calculate_embeddings(image_path)\n",
        "        min_distance = float('inf')\n",
        "        min_class = None\n",
        "        for subdir in os.listdir(main_dir):\n",
        "            subdir_path = os.path.join(main_dir, subdir)\n",
        "            for img_file in os.listdir(subdir_path):\n",
        "                img_path = os.path.join(subdir_path, img_file)\n",
        "                embeddings = calculate_embeddings(img_path)\n",
        "                distance = np.linalg.norm(uploaded_embeddings - embeddings)\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    min_class = subdir\n",
        "        result_var.set(f\"The song clip belongs to class: {min_class}\")\n",
        "\n",
        "# Create tkinter window\n",
        "root = tk.Tk()\n",
        "root.title(\"Image Classification\")\n",
        "\n",
        "# Create widgets\n",
        "browse_button = tk.Button(root, text=\"Upload Image\", command=browse_file)\n",
        "browse_button.pack(pady=10)\n",
        "\n",
        "image_label = tk.Label(root)\n",
        "image_label.pack(pady=10)\n",
        "\n",
        "image_path_var = tk.StringVar()\n",
        "image_path_entry = tk.Entry(root, textvariable=image_path_var, state='readonly')\n",
        "image_path_entry.pack(pady=10)\n",
        "\n",
        "classify_button = tk.Button(root, text=\"Classify Image\", command=match_embeddings)\n",
        "classify_button.pack(pady=10)\n",
        "\n",
        "result_var = tk.StringVar()\n",
        "result_label = tk.Label(root, textvariable=result_var)\n",
        "result_label.pack(pady=10)\n",
        "\n",
        "# Function to close the tkinter window\n",
        "def close_window():\n",
        "    root.destroy()\n",
        "\n",
        "# Bind closing event to close_window function\n",
        "root.protocol(\"WM_DELETE_WINDOW\", close_window)\n",
        "\n",
        "# Run the tkinter event loop\n",
        "root.mainloop()\n"
      ],
      "metadata": {
        "id": "aOeJv_2_7XVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0456f562-2d2a-48d9-888e-7d77c341ab76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 230 images belonging to 10 classes.\n",
            "Found 50 images belonging to 10 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258076736/258076736 [==============================] - 2s 0us/step\n",
            "Epoch 1/50\n",
            "29/29 [==============================] - 64s 1s/step - loss: 2.8022 - accuracy: 0.1000 - val_loss: 2.5137 - val_accuracy: 0.1000\n",
            "Epoch 2/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.6509 - accuracy: 0.0826 - val_loss: 2.6999 - val_accuracy: 0.1000\n",
            "Epoch 3/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5790 - accuracy: 0.1174 - val_loss: 2.5645 - val_accuracy: 0.1000\n",
            "Epoch 4/50\n",
            "29/29 [==============================] - 36s 1s/step - loss: 2.6844 - accuracy: 0.0391 - val_loss: 2.4721 - val_accuracy: 0.1000\n",
            "Epoch 5/50\n",
            "29/29 [==============================] - 33s 1s/step - loss: 2.5448 - accuracy: 0.1043 - val_loss: 2.5373 - val_accuracy: 0.1000\n",
            "Epoch 6/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5614 - accuracy: 0.1087 - val_loss: 2.5146 - val_accuracy: 0.1000\n",
            "Epoch 7/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.6697 - accuracy: 0.0739 - val_loss: 2.3777 - val_accuracy: 0.1000\n",
            "Epoch 8/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.4811 - accuracy: 0.0870 - val_loss: 2.4142 - val_accuracy: 0.1000\n",
            "Epoch 9/50\n",
            "29/29 [==============================] - 34s 1s/step - loss: 2.4614 - accuracy: 0.0913 - val_loss: 2.3614 - val_accuracy: 0.1000\n",
            "Epoch 10/50\n",
            "29/29 [==============================] - 34s 1s/step - loss: 2.4516 - accuracy: 0.1348 - val_loss: 2.6145 - val_accuracy: 0.1000\n",
            "Epoch 11/50\n",
            "29/29 [==============================] - 33s 1s/step - loss: 2.6644 - accuracy: 0.0913 - val_loss: 2.4050 - val_accuracy: 0.1400\n",
            "Epoch 12/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.6647 - accuracy: 0.0609 - val_loss: 2.4065 - val_accuracy: 0.1600\n",
            "Epoch 13/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5160 - accuracy: 0.0739 - val_loss: 2.4996 - val_accuracy: 0.1000\n",
            "Epoch 14/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5505 - accuracy: 0.0957 - val_loss: 2.5333 - val_accuracy: 0.1000\n",
            "Epoch 15/50\n",
            "29/29 [==============================] - 34s 1s/step - loss: 2.4862 - accuracy: 0.0957 - val_loss: 2.3985 - val_accuracy: 0.1000\n",
            "Epoch 16/50\n",
            "29/29 [==============================] - 36s 1s/step - loss: 2.5446 - accuracy: 0.0826 - val_loss: 2.5411 - val_accuracy: 0.1000\n",
            "Epoch 17/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.6589 - accuracy: 0.0739 - val_loss: 2.6539 - val_accuracy: 0.1000\n",
            "Epoch 18/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.4874 - accuracy: 0.1043 - val_loss: 2.6033 - val_accuracy: 0.1000\n",
            "Epoch 19/50\n",
            "29/29 [==============================] - 35s 1s/step - loss: 2.6478 - accuracy: 0.1043 - val_loss: 2.5670 - val_accuracy: 0.1800\n",
            "Epoch 20/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.6677 - accuracy: 0.1087 - val_loss: 2.6546 - val_accuracy: 0.1000\n",
            "Epoch 21/50\n",
            "29/29 [==============================] - 34s 1s/step - loss: 2.5166 - accuracy: 0.1174 - val_loss: 2.4677 - val_accuracy: 0.1000\n",
            "Epoch 22/50\n",
            "29/29 [==============================] - 33s 1s/step - loss: 2.5374 - accuracy: 0.0696 - val_loss: 2.5187 - val_accuracy: 0.1000\n",
            "Epoch 23/50\n",
            "29/29 [==============================] - 31s 1s/step - loss: 2.6150 - accuracy: 0.1000 - val_loss: 2.6469 - val_accuracy: 0.1000\n",
            "Epoch 24/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5889 - accuracy: 0.0957 - val_loss: 2.4687 - val_accuracy: 0.1000\n",
            "Epoch 25/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.5281 - accuracy: 0.0826 - val_loss: 2.3905 - val_accuracy: 0.1000\n",
            "Epoch 26/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.4892 - accuracy: 0.1087 - val_loss: 2.3534 - val_accuracy: 0.1000\n",
            "Epoch 27/50\n",
            "29/29 [==============================] - 34s 1s/step - loss: 2.4933 - accuracy: 0.0913 - val_loss: 2.4264 - val_accuracy: 0.1000\n",
            "Epoch 28/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.6332 - accuracy: 0.0913 - val_loss: 2.5837 - val_accuracy: 0.1000\n",
            "Epoch 29/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.4747 - accuracy: 0.1043 - val_loss: 2.3533 - val_accuracy: 0.1000\n",
            "Epoch 30/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.6375 - accuracy: 0.1130 - val_loss: 2.5548 - val_accuracy: 0.1000\n",
            "Epoch 31/50\n",
            "29/29 [==============================] - 36s 1s/step - loss: 2.5219 - accuracy: 0.0565 - val_loss: 2.4976 - val_accuracy: 0.1000\n",
            "Epoch 32/50\n",
            "29/29 [==============================] - 33s 1s/step - loss: 2.5647 - accuracy: 0.1217 - val_loss: 2.4369 - val_accuracy: 0.1000\n",
            "Epoch 33/50\n",
            "29/29 [==============================] - 29s 1s/step - loss: 2.5637 - accuracy: 0.0739 - val_loss: 2.5792 - val_accuracy: 0.1600\n",
            "Epoch 34/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.5323 - accuracy: 0.0870 - val_loss: 2.4482 - val_accuracy: 0.1000\n",
            "Epoch 35/50\n",
            "29/29 [==============================] - 35s 1s/step - loss: 2.4615 - accuracy: 0.1261 - val_loss: 2.6448 - val_accuracy: 0.1000\n",
            "Epoch 36/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.6074 - accuracy: 0.0957 - val_loss: 2.4287 - val_accuracy: 0.1000\n",
            "Epoch 37/50\n",
            "29/29 [==============================] - 33s 1s/step - loss: 2.4591 - accuracy: 0.1043 - val_loss: 2.5017 - val_accuracy: 0.1000\n",
            "Epoch 38/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.5198 - accuracy: 0.0913 - val_loss: 2.5568 - val_accuracy: 0.1000\n",
            "Epoch 39/50\n",
            "29/29 [==============================] - 30s 1s/step - loss: 2.5867 - accuracy: 0.0957 - val_loss: 2.4978 - val_accuracy: 0.1000\n",
            "Epoch 40/50\n",
            "29/29 [==============================] - 31s 1s/step - loss: 2.5390 - accuracy: 0.1304 - val_loss: 2.4240 - val_accuracy: 0.1000\n",
            "Epoch 41/50\n",
            " 9/29 [========>.....................] - ETA: 16s - loss: 2.3168 - accuracy: 0.1389"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqJO9D92KVV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}