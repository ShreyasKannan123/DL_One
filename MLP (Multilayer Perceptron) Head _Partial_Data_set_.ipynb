{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aonXwJ2SRx7i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3J_j3qWKtCT",
    "outputId": "a8b86c1b-3a30-4ce3-a616-bec97e4dfef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder images already exists.\n",
      "Files extracted to images successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the ZIP file\n",
    "zip_file_path = 'fma_img.zip'\n",
    "\n",
    "# Define the path to the folder where you want to extract the files\n",
    "extract_folder_path = 'images'\n",
    "\n",
    "try:\n",
    "    # Create the extract folder if it doesn't exist\n",
    "    if not os.path.exists(extract_folder_path):\n",
    "        os.makedirs(extract_folder_path)\n",
    "        print(f\"Folder {extract_folder_path} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder {extract_folder_path} already exists.\")\n",
    "\n",
    "    # Extract the ZIP file to the extract folder\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder_path)\n",
    "    print(f\"Files extracted to {extract_folder_path} successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {zip_file_path} was not found.\")\n",
    "except PermissionError:\n",
    "    print(f\"Error: You don't have permission to create the folder {extract_folder_path} or extract the ZIP file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JMVaewFYLBZj",
    "outputId": "e8abda45-3635-466d-e984-859922c0a2ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, LayerNormalization, MultiHeadAttention, Dense, GlobalAveragePooling1D, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path to the main directory containing subdirectories with images\n",
    "\n",
    "main_dir = r'C:\\Users\\Plaksha\\3D Objects\\DL\\fma_img\\fma_img'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230 images belonging to 13 classes.\n",
      "Found 50 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define image dimensions and batch size\n",
    "img_height, img_width = 100, 100\n",
    "batch_size = 16\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # Splitting data for validation\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7tAN2BUYLGSN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 98, 98, 32)           896       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 49, 49, 32)           0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 2401, 32)             0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 2401, 32)             64        ['reshape_2[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 2401, 32)             8416      ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 2401, 32)             0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 2401, 64)             2112      ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 2401, 64)             0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 2401, 64)             0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 2401, 64)             4160      ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 2401, 64)             0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 64)                   0         ['activation_5[0][0]']        \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 13)                   845       ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16493 (64.43 KB)\n",
      "Trainable params: 16493 (64.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "15/15 [==============================] - 81s 5s/step - loss: 2.5366 - accuracy: 0.0783 - val_loss: 2.4466 - val_accuracy: 0.1600\n",
      "Epoch 2/40\n",
      "15/15 [==============================] - 112s 8s/step - loss: 2.4516 - accuracy: 0.1043 - val_loss: 2.3591 - val_accuracy: 0.2000\n",
      "Epoch 3/40\n",
      "15/15 [==============================] - 153s 10s/step - loss: 2.4056 - accuracy: 0.0826 - val_loss: 2.3393 - val_accuracy: 0.1000\n",
      "Epoch 4/40\n",
      "15/15 [==============================] - 178s 12s/step - loss: 2.3902 - accuracy: 0.0783 - val_loss: 2.3199 - val_accuracy: 0.1200\n",
      "Epoch 5/40\n",
      "15/15 [==============================] - 205s 13s/step - loss: 2.3868 - accuracy: 0.1087 - val_loss: 2.3108 - val_accuracy: 0.1400\n",
      "Epoch 6/40\n",
      "15/15 [==============================] - 155s 10s/step - loss: 2.3754 - accuracy: 0.1174 - val_loss: 2.3175 - val_accuracy: 0.1600\n",
      "Epoch 7/40\n",
      "15/15 [==============================] - 153s 10s/step - loss: 2.3743 - accuracy: 0.1174 - val_loss: 2.2962 - val_accuracy: 0.2000\n",
      "Epoch 8/40\n",
      "15/15 [==============================] - 100s 6s/step - loss: 2.3688 - accuracy: 0.1304 - val_loss: 2.2671 - val_accuracy: 0.1200\n",
      "Epoch 9/40\n",
      "15/15 [==============================] - 148s 10s/step - loss: 2.3542 - accuracy: 0.1348 - val_loss: 2.2736 - val_accuracy: 0.1600\n",
      "Epoch 10/40\n",
      "15/15 [==============================] - 138s 9s/step - loss: 2.3141 - accuracy: 0.0870 - val_loss: 2.2249 - val_accuracy: 0.1200\n",
      "Epoch 11/40\n",
      "15/15 [==============================] - 133s 9s/step - loss: 2.2922 - accuracy: 0.1478 - val_loss: 2.1899 - val_accuracy: 0.1800\n",
      "Epoch 12/40\n",
      "15/15 [==============================] - 141s 9s/step - loss: 2.2789 - accuracy: 0.1913 - val_loss: 2.1258 - val_accuracy: 0.2200\n",
      "Epoch 13/40\n",
      "15/15 [==============================] - 128s 9s/step - loss: 2.2782 - accuracy: 0.1957 - val_loss: 2.1475 - val_accuracy: 0.2600\n",
      "Epoch 14/40\n",
      "15/15 [==============================] - 138s 9s/step - loss: 2.2612 - accuracy: 0.1913 - val_loss: 2.1094 - val_accuracy: 0.1800\n",
      "Epoch 15/40\n",
      "15/15 [==============================] - 139s 9s/step - loss: 2.1750 - accuracy: 0.2174 - val_loss: 2.0652 - val_accuracy: 0.2200\n",
      "Epoch 16/40\n",
      "15/15 [==============================] - 134s 9s/step - loss: 2.0862 - accuracy: 0.2826 - val_loss: 2.0686 - val_accuracy: 0.2200\n",
      "Epoch 17/40\n",
      "15/15 [==============================] - 134s 9s/step - loss: 1.9850 - accuracy: 0.3217 - val_loss: 2.1691 - val_accuracy: 0.1400\n",
      "Epoch 18/40\n",
      "15/15 [==============================] - 125s 8s/step - loss: 1.9239 - accuracy: 0.3174 - val_loss: 2.1807 - val_accuracy: 0.2400\n",
      "Epoch 19/40\n",
      "15/15 [==============================] - 127s 9s/step - loss: 1.8546 - accuracy: 0.3391 - val_loss: 1.9884 - val_accuracy: 0.2200\n",
      "Epoch 20/40\n",
      "15/15 [==============================] - 124s 8s/step - loss: 1.8328 - accuracy: 0.3522 - val_loss: 1.9716 - val_accuracy: 0.3200\n",
      "Epoch 21/40\n",
      "15/15 [==============================] - 127s 9s/step - loss: 1.8583 - accuracy: 0.3348 - val_loss: 2.2544 - val_accuracy: 0.2200\n",
      "Epoch 22/40\n",
      "15/15 [==============================] - 126s 8s/step - loss: 1.7986 - accuracy: 0.3348 - val_loss: 2.0906 - val_accuracy: 0.3000\n",
      "Epoch 23/40\n",
      "15/15 [==============================] - 126s 8s/step - loss: 1.7047 - accuracy: 0.3696 - val_loss: 1.8869 - val_accuracy: 0.2600\n",
      "Epoch 24/40\n",
      "15/15 [==============================] - 124s 8s/step - loss: 1.6232 - accuracy: 0.4565 - val_loss: 2.2024 - val_accuracy: 0.3000\n",
      "Epoch 25/40\n",
      "15/15 [==============================] - 125s 8s/step - loss: 1.5527 - accuracy: 0.4783 - val_loss: 1.8980 - val_accuracy: 0.2600\n",
      "Epoch 26/40\n",
      "15/15 [==============================] - 126s 9s/step - loss: 1.5060 - accuracy: 0.4565 - val_loss: 2.2384 - val_accuracy: 0.2000\n",
      "Epoch 27/40\n",
      "15/15 [==============================] - 133s 9s/step - loss: 1.5076 - accuracy: 0.4783 - val_loss: 1.9968 - val_accuracy: 0.3000\n",
      "Epoch 28/40\n",
      "15/15 [==============================] - 146s 10s/step - loss: 1.4219 - accuracy: 0.5130 - val_loss: 2.1263 - val_accuracy: 0.2400\n",
      "Epoch 29/40\n",
      "15/15 [==============================] - 129s 9s/step - loss: 1.3489 - accuracy: 0.5087 - val_loss: 2.1123 - val_accuracy: 0.2600\n",
      "Epoch 30/40\n",
      "15/15 [==============================] - 125s 8s/step - loss: 1.2498 - accuracy: 0.6043 - val_loss: 1.9817 - val_accuracy: 0.3200\n",
      "Epoch 31/40\n",
      "15/15 [==============================] - 119s 8s/step - loss: 1.1903 - accuracy: 0.5783 - val_loss: 2.4413 - val_accuracy: 0.3000\n",
      "Epoch 32/40\n",
      "15/15 [==============================] - 126s 8s/step - loss: 1.0540 - accuracy: 0.6348 - val_loss: 2.3420 - val_accuracy: 0.3000\n",
      "Epoch 33/40\n",
      "15/15 [==============================] - 130s 9s/step - loss: 0.9941 - accuracy: 0.6826 - val_loss: 2.3134 - val_accuracy: 0.2800\n",
      "Epoch 34/40\n",
      "15/15 [==============================] - 129s 9s/step - loss: 0.9044 - accuracy: 0.7087 - val_loss: 2.5934 - val_accuracy: 0.2400\n",
      "Epoch 35/40\n",
      "15/15 [==============================] - 127s 8s/step - loss: 0.9473 - accuracy: 0.6609 - val_loss: 3.0220 - val_accuracy: 0.3400\n",
      "Epoch 36/40\n",
      "15/15 [==============================] - 137s 9s/step - loss: 1.0487 - accuracy: 0.6043 - val_loss: 2.6880 - val_accuracy: 0.3800\n",
      "Epoch 37/40\n",
      "15/15 [==============================] - 134s 9s/step - loss: 0.9160 - accuracy: 0.6696 - val_loss: 2.7067 - val_accuracy: 0.3800\n",
      "Epoch 38/40\n",
      "15/15 [==============================] - 125s 8s/step - loss: 0.7785 - accuracy: 0.7217 - val_loss: 2.8922 - val_accuracy: 0.3400\n",
      "Epoch 39/40\n",
      "15/15 [==============================] - 123s 8s/step - loss: 0.8618 - accuracy: 0.6957 - val_loss: 2.8571 - val_accuracy: 0.3600\n",
      "Epoch 40/40\n",
      "15/15 [==============================] - 95s 7s/step - loss: 0.8643 - accuracy: 0.6913 - val_loss: 2.8381 - val_accuracy: 0.3400\n",
      "Training Accuracy: 0.6913043260574341\n",
      "Validation Accuracy: 0.3400000035762787\n",
      "Training Loss: 0.8643189668655396\n",
      "Validation Loss: 2.8381173610687256\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Multi-Head Attention\n",
    "num_heads = 2  # Number of attention heads\n",
    "head_size = 32  # Dimension of each attention head\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Build the model with an MLP head\n",
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Reshape(target_shape=(-1, x.shape[-1]))(x)  # Flatten for attention\n",
    "x = LayerNormalization(epsilon=1e-6)(x)\n",
    "x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout_rate)(x, x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(ff_dim)(x)\n",
    "x = Activation(gelu)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(ff_dim)(x)\n",
    "x = Activation(gelu)(x)\n",
    "x = GlobalAveragePooling1D()(x)  # Using 1D pooling for flattened features\n",
    "outputs = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=40, validation_data=validation_generator)\n",
    "\n",
    "# Display training results\n",
    "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n",
    "print(\"Training Loss:\", history.history['loss'][-1])\n",
    "print(\"Validation Loss:\", history.history['val_loss'][-1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the uploaded image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def calculate_embeddings(image_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    embeddings = model.predict(img_array)\n",
    "    return embeddings\n",
    "\n",
    "# Function to open file dialog and get the image path\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    image_path_var.set(file_path)\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((200, 200), Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        image_label.config(image=img)\n",
    "        image_label.image = img\n",
    "\n",
    "# Function to match embeddings and determine class\n",
    "def match_embeddings():\n",
    "    image_path = image_path_var.get()\n",
    "    if image_path:\n",
    "        uploaded_embeddings = calculate_embeddings(image_path)\n",
    "        min_distance = float('inf')\n",
    "        min_class = None\n",
    "        for subdir in os.listdir(main_dir):\n",
    "            subdir_path = os.path.join(main_dir, subdir)\n",
    "            for img_file in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, img_file)\n",
    "                embeddings = calculate_embeddings(img_path)\n",
    "                distance = np.linalg.norm(uploaded_embeddings - embeddings)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_class = subdir\n",
    "        result_var.set(f\"The song clip belongs to class: {min_class}\")\n",
    "\n",
    "# Create tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image Classification\")\n",
    "\n",
    "# Create widgets\n",
    "browse_button = tk.Button(root, text=\"Upload Image\", command=browse_file)\n",
    "browse_button.pack(pady=10)\n",
    "\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "image_path_var = tk.StringVar()\n",
    "image_path_entry = tk.Entry(root, textvariable=image_path_var, state='readonly')\n",
    "image_path_entry.pack(pady=10)\n",
    "\n",
    "classify_button = tk.Button(root, text=\"Classify Image\", command=match_embeddings)\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "result_var = tk.StringVar()\n",
    "result_label = tk.Label(root, textvariable=result_var)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Function to close the tkinter window\n",
    "def close_window():\n",
    "    root.destroy()\n",
    "\n",
    "# Bind closing event to close_window function\n",
    "root.protocol(\"WM_DELETE_WINDOW\", close_window)\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
