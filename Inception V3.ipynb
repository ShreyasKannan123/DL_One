{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff8cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2c5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the main directory containing subdirectories with images\n",
    "main_dir = r'C:\\Users\\Plaksha\\3D Objects\\DL\\fma_img\\fma_img'\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f42ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230 images belonging to 13 classes.\n",
      "Found 50 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use subset parameter for training data\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use subset parameter for validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c00b9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 30s 885ms/step - loss: 2.6931 - accuracy: 0.1348 - val_loss: 13.3105 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 12s 779ms/step - loss: 2.1631 - accuracy: 0.2783 - val_loss: 7.4073 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 12s 824ms/step - loss: 1.9626 - accuracy: 0.3826 - val_loss: 9.9717 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 13s 862ms/step - loss: 1.6458 - accuracy: 0.5174 - val_loss: 2.3060 - val_accuracy: 0.1600\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 12s 808ms/step - loss: 1.6061 - accuracy: 0.5130 - val_loss: 2.2816 - val_accuracy: 0.1400\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 12s 808ms/step - loss: 1.4468 - accuracy: 0.5739 - val_loss: 2.7666 - val_accuracy: 0.3000\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 13s 837ms/step - loss: 1.1212 - accuracy: 0.7217 - val_loss: 2.2055 - val_accuracy: 0.3000\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 12s 813ms/step - loss: 1.7253 - accuracy: 0.5348 - val_loss: 430.6997 - val_accuracy: 0.1400\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 12s 790ms/step - loss: 1.2197 - accuracy: 0.6391 - val_loss: 3.6844 - val_accuracy: 0.2400\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 12s 798ms/step - loss: 1.0897 - accuracy: 0.6696 - val_loss: 4.5982 - val_accuracy: 0.1800\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 12s 788ms/step - loss: 1.1644 - accuracy: 0.6870 - val_loss: 5.0575 - val_accuracy: 0.1800\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 14s 921ms/step - loss: 1.3531 - accuracy: 0.6870 - val_loss: 9.1573 - val_accuracy: 0.2000\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 15s 981ms/step - loss: 1.4925 - accuracy: 0.5739 - val_loss: 7.9128 - val_accuracy: 0.1400\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 12s 814ms/step - loss: 0.9512 - accuracy: 0.7217 - val_loss: 61.0731 - val_accuracy: 0.2200\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 12s 791ms/step - loss: 1.3016 - accuracy: 0.6783 - val_loss: 3.9455 - val_accuracy: 0.1400\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 12s 786ms/step - loss: 1.1118 - accuracy: 0.6783 - val_loss: 22.7600 - val_accuracy: 0.2200\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 12s 819ms/step - loss: 1.0382 - accuracy: 0.6609 - val_loss: 27.6295 - val_accuracy: 0.1600\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 12s 836ms/step - loss: 0.9092 - accuracy: 0.7609 - val_loss: 1.8731 - val_accuracy: 0.3800\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 12s 791ms/step - loss: 0.7432 - accuracy: 0.7652 - val_loss: 19.3137 - val_accuracy: 0.3400\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 12s 781ms/step - loss: 0.7979 - accuracy: 0.7522 - val_loss: 5523.8286 - val_accuracy: 0.1000\n",
      "Training Accuracy: 0.752173900604248\n",
      "Validation Accuracy: 0.10000000149011612\n",
      "Training Loss: 0.7979283928871155\n",
      "Validation Loss: 5523.82861328125\n",
      "4/4 [==============================] - 2s 94ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "base_model = InceptionV3(include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=20, validation_data=validation_generator)\n",
    "\n",
    "# Print accuracy and other performance metrics\n",
    "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n",
    "print(\"Training Loss:\", history.history['loss'][-1])\n",
    "print(\"Validation Loss:\", history.history['val_loss'][-1])\n",
    "\n",
    "# Evaluate the model\n",
    "validation_steps = len(validation_generator)\n",
    "predictions = model.predict(validation_generator, steps=validation_steps)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "# Print classification report with specified labels parameter\n",
    "# print(classification_report(true_classes, predicted_classes, labels=np.unique(true_classes), target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86667f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the uploaded image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def calculate_embeddings(image_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    embeddings = model.predict(img_array)\n",
    "    return embeddings\n",
    "\n",
    "# Function to open file dialog and get the image path\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    image_path_var.set(file_path)\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((200, 200), Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        image_label.config(image=img)\n",
    "        image_label.image = img\n",
    "\n",
    "# Function to match embeddings and determine class\n",
    "def match_embeddings():\n",
    "    image_path = image_path_var.get()\n",
    "    if image_path:\n",
    "        uploaded_embeddings = calculate_embeddings(image_path)\n",
    "        min_distance = float('inf')\n",
    "        min_class = None\n",
    "        for subdir in os.listdir(main_dir):\n",
    "            subdir_path = os.path.join(main_dir, subdir)\n",
    "            for img_file in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, img_file)\n",
    "                embeddings = calculate_embeddings(img_path)\n",
    "                distance = np.linalg.norm(uploaded_embeddings - embeddings)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_class = subdir\n",
    "        result_var.set(f\"The song clip belongs to class: {min_class}\")\n",
    "\n",
    "# Create tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image Classification\")\n",
    "\n",
    "# Create widgets\n",
    "browse_button = tk.Button(root, text=\"Upload Image\", command=browse_file)\n",
    "browse_button.pack(pady=10)\n",
    "\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "image_path_var = tk.StringVar()\n",
    "image_path_entry = tk.Entry(root, textvariable=image_path_var, state='readonly')\n",
    "image_path_entry.pack(pady=10)\n",
    "\n",
    "classify_button = tk.Button(root, text=\"Classify Image\", command=match_embeddings)\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "result_var = tk.StringVar()\n",
    "result_label = tk.Label(root, textvariable=result_var)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Function to close the tkinter window\n",
    "def close_window():\n",
    "    root.destroy()\n",
    "\n",
    "# Bind closing event to close_window function\n",
    "root.protocol(\"WM_DELETE_WINDOW\", close_window)\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
