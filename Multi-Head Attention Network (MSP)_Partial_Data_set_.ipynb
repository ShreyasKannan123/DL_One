{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TnphA-iX5_8P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, LayerNormalization, MultiHeadAttention, Dense, GlobalAveragePooling1D, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path to the main directory containing subdirectories with images\n",
    "\n",
    "main_dir = r'C:\\Users\\Plaksha\\3D Objects\\DL\\fma_img\\fma_img'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWItk3RF6A71",
    "outputId": "d844cc5f-7465-4f50-f621-2fdd5b3207ce"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TRbYI7j8_UD2",
    "outputId": "904a6bf2-475d-4056-87e6-f62fc6ab4db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230 images belonging to 13 classes.\n",
      "Found 50 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define image dimensions and batch size\n",
    "img_height, img_width = 100, 100\n",
    "batch_size = 8\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # Splitting data for validation\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    main_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0V2dHOTcBtx5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 98, 98, 32)           896       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 49, 49, 32)           0         ['conv2d_1[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 2401, 32)             0         ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 2401, 32)             64        ['reshape_1[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 2401, 32)             8416      ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 2401, 32)             0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 2401, 64)             2112      ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 2401, 64)             0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 2401, 3)              195       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 3)                    0         ['dense_4[0][0]']             \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 13)                   52        ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11735 (45.84 KB)\n",
      "Trainable params: 11735 (45.84 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "29/29 [==============================] - 177s 6s/step - loss: 2.5709 - accuracy: 0.0957 - val_loss: 2.5513 - val_accuracy: 0.1000\n",
      "Epoch 2/40\n",
      "29/29 [==============================] - 206s 7s/step - loss: 2.5510 - accuracy: 0.0739 - val_loss: 2.5407 - val_accuracy: 0.1000\n",
      "Epoch 3/40\n",
      "29/29 [==============================] - 171s 6s/step - loss: 2.5452 - accuracy: 0.0739 - val_loss: 2.5282 - val_accuracy: 0.1000\n",
      "Epoch 4/40\n",
      "29/29 [==============================] - 154s 5s/step - loss: 2.5290 - accuracy: 0.0913 - val_loss: 2.5141 - val_accuracy: 0.1000\n",
      "Epoch 5/40\n",
      "29/29 [==============================] - 143s 5s/step - loss: 2.5176 - accuracy: 0.0783 - val_loss: 2.5022 - val_accuracy: 0.1000\n",
      "Epoch 6/40\n",
      "29/29 [==============================] - 146s 5s/step - loss: 2.5006 - accuracy: 0.1000 - val_loss: 2.4803 - val_accuracy: 0.1000\n",
      "Epoch 7/40\n",
      "29/29 [==============================] - 134s 5s/step - loss: 2.4869 - accuracy: 0.1000 - val_loss: 2.4669 - val_accuracy: 0.1000\n",
      "Epoch 8/40\n",
      "29/29 [==============================] - 130s 5s/step - loss: 2.4762 - accuracy: 0.1000 - val_loss: 2.4517 - val_accuracy: 0.1000\n",
      "Epoch 9/40\n",
      "29/29 [==============================] - 136s 5s/step - loss: 2.4647 - accuracy: 0.1000 - val_loss: 2.4395 - val_accuracy: 0.1000\n",
      "Epoch 10/40\n",
      "29/29 [==============================] - 124s 4s/step - loss: 2.4472 - accuracy: 0.1000 - val_loss: 2.4225 - val_accuracy: 0.1000\n",
      "Epoch 11/40\n",
      "29/29 [==============================] - 134s 5s/step - loss: 2.4417 - accuracy: 0.1000 - val_loss: 2.4068 - val_accuracy: 0.1800\n",
      "Epoch 12/40\n",
      "29/29 [==============================] - 136s 5s/step - loss: 2.4209 - accuracy: 0.1087 - val_loss: 2.4032 - val_accuracy: 0.1000\n",
      "Epoch 13/40\n",
      "29/29 [==============================] - 133s 5s/step - loss: 2.3801 - accuracy: 0.1087 - val_loss: 2.3341 - val_accuracy: 0.1000\n",
      "Epoch 14/40\n",
      "29/29 [==============================] - 127s 4s/step - loss: 2.3330 - accuracy: 0.1261 - val_loss: 2.2803 - val_accuracy: 0.1200\n",
      "Epoch 15/40\n",
      "29/29 [==============================] - 125s 4s/step - loss: 2.2606 - accuracy: 0.1696 - val_loss: 2.3719 - val_accuracy: 0.1000\n",
      "Epoch 16/40\n",
      "29/29 [==============================] - 121s 4s/step - loss: 2.1977 - accuracy: 0.1565 - val_loss: 2.2380 - val_accuracy: 0.1400\n",
      "Epoch 17/40\n",
      "29/29 [==============================] - 125s 4s/step - loss: 2.1448 - accuracy: 0.1870 - val_loss: 2.1700 - val_accuracy: 0.2000\n",
      "Epoch 18/40\n",
      "29/29 [==============================] - 118s 4s/step - loss: 2.1268 - accuracy: 0.2087 - val_loss: 2.1645 - val_accuracy: 0.1800\n",
      "Epoch 19/40\n",
      "29/29 [==============================] - 124s 4s/step - loss: 2.0315 - accuracy: 0.2652 - val_loss: 2.1252 - val_accuracy: 0.2800\n",
      "Epoch 20/40\n",
      "29/29 [==============================] - 121s 4s/step - loss: 1.9793 - accuracy: 0.2435 - val_loss: 2.2377 - val_accuracy: 0.3000\n",
      "Epoch 21/40\n",
      "29/29 [==============================] - 120s 4s/step - loss: 2.0121 - accuracy: 0.2348 - val_loss: 2.1010 - val_accuracy: 0.2800\n",
      "Epoch 22/40\n",
      "29/29 [==============================] - 124s 4s/step - loss: 1.9056 - accuracy: 0.3087 - val_loss: 1.9413 - val_accuracy: 0.3000\n",
      "Epoch 23/40\n",
      "29/29 [==============================] - 121s 4s/step - loss: 1.9602 - accuracy: 0.2565 - val_loss: 2.1099 - val_accuracy: 0.2800\n",
      "Epoch 24/40\n",
      "29/29 [==============================] - 121s 4s/step - loss: 1.8376 - accuracy: 0.2870 - val_loss: 1.9504 - val_accuracy: 0.3000\n",
      "Epoch 25/40\n",
      "29/29 [==============================] - 144s 5s/step - loss: 1.7645 - accuracy: 0.3304 - val_loss: 1.9720 - val_accuracy: 0.3200\n",
      "Epoch 26/40\n",
      "29/29 [==============================] - 128s 4s/step - loss: 1.7198 - accuracy: 0.3522 - val_loss: 2.2669 - val_accuracy: 0.2800\n",
      "Epoch 27/40\n",
      "29/29 [==============================] - 127s 4s/step - loss: 1.7010 - accuracy: 0.3652 - val_loss: 2.0631 - val_accuracy: 0.3200\n",
      "Epoch 28/40\n",
      "29/29 [==============================] - 120s 4s/step - loss: 1.7188 - accuracy: 0.3826 - val_loss: 2.3090 - val_accuracy: 0.2200\n",
      "Epoch 29/40\n",
      "29/29 [==============================] - 117s 4s/step - loss: 1.6824 - accuracy: 0.4130 - val_loss: 1.9937 - val_accuracy: 0.2600\n",
      "Epoch 30/40\n",
      "29/29 [==============================] - 119s 4s/step - loss: 1.5217 - accuracy: 0.4087 - val_loss: 2.4032 - val_accuracy: 0.2600\n",
      "Epoch 31/40\n",
      "29/29 [==============================] - 126s 4s/step - loss: 1.5785 - accuracy: 0.4435 - val_loss: 2.2213 - val_accuracy: 0.2200\n",
      "Epoch 32/40\n",
      "29/29 [==============================] - 123s 4s/step - loss: 1.4791 - accuracy: 0.4391 - val_loss: 2.1568 - val_accuracy: 0.2600\n",
      "Epoch 33/40\n",
      "29/29 [==============================] - 126s 4s/step - loss: 1.3057 - accuracy: 0.5130 - val_loss: 2.1819 - val_accuracy: 0.3400\n",
      "Epoch 34/40\n",
      "29/29 [==============================] - 131s 5s/step - loss: 1.4600 - accuracy: 0.5043 - val_loss: 2.3426 - val_accuracy: 0.2600\n",
      "Epoch 35/40\n",
      "29/29 [==============================] - 127s 4s/step - loss: 1.4494 - accuracy: 0.4652 - val_loss: 2.1045 - val_accuracy: 0.3600\n",
      "Epoch 36/40\n",
      "29/29 [==============================] - 122s 4s/step - loss: 1.3072 - accuracy: 0.5000 - val_loss: 2.2362 - val_accuracy: 0.3600\n",
      "Epoch 37/40\n",
      "29/29 [==============================] - 117s 4s/step - loss: 1.2113 - accuracy: 0.5261 - val_loss: 2.4466 - val_accuracy: 0.3400\n",
      "Epoch 38/40\n",
      "29/29 [==============================] - 90s 3s/step - loss: 1.1846 - accuracy: 0.5174 - val_loss: 2.2884 - val_accuracy: 0.4400\n",
      "Epoch 39/40\n",
      "29/29 [==============================] - 49s 2s/step - loss: 1.1826 - accuracy: 0.5435 - val_loss: 2.6281 - val_accuracy: 0.3000\n",
      "Epoch 40/40\n",
      "29/29 [==============================] - 49s 2s/step - loss: 1.2376 - accuracy: 0.5087 - val_loss: 2.5756 - val_accuracy: 0.2400\n",
      "Training Accuracy: 0.508695662021637\n",
      "Validation Accuracy: 0.23999999463558197\n",
      "Training Loss: 1.2375555038452148\n",
      "Validation Loss: 2.575634241104126\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Multi-Head Attention\n",
    "num_heads = 2  # Number of attention heads\n",
    "head_size = 32  # Dimension of each attention head\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Build the model\n",
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Reshape(target_shape=(-1, x.shape[-1]))(x)  # Flatten for attention\n",
    "x = LayerNormalization(epsilon=1e-6)(x)\n",
    "x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout_rate)(x, x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(ff_dim, activation='relu')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(inputs.shape[-1])(x)\n",
    "x = GlobalAveragePooling1D()(x)  # Using 1D pooling for flattened features\n",
    "outputs = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=40, validation_data=validation_generator)\n",
    "\n",
    "# Display training results\n",
    "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n",
    "print(\"Training Loss:\", history.history['loss'][-1])\n",
    "print(\"Validation Loss:\", history.history['val_loss'][-1])\n",
    "\n",
    "# The rest of the code for GUI and file operations remains the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the uploaded image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def calculate_embeddings(image_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    embeddings = model.predict(img_array)\n",
    "    return embeddings\n",
    "\n",
    "# Function to open file dialog and get the image path\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    image_path_var.set(file_path)\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((200, 200), Image.ANTIALIAS)\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        image_label.config(image=img)\n",
    "        image_label.image = img\n",
    "\n",
    "# Function to match embeddings and determine class\n",
    "def match_embeddings():\n",
    "    image_path = image_path_var.get()\n",
    "    if image_path:\n",
    "        uploaded_embeddings = calculate_embeddings(image_path)\n",
    "        min_distance = float('inf')\n",
    "        min_class = None\n",
    "        for subdir in os.listdir(main_dir):\n",
    "            subdir_path = os.path.join(main_dir, subdir)\n",
    "            for img_file in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, img_file)\n",
    "                embeddings = calculate_embeddings(img_path)\n",
    "                distance = np.linalg.norm(uploaded_embeddings - embeddings)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_class = subdir\n",
    "        result_var.set(f\"The song clip belongs to class: {min_class}\")\n",
    "\n",
    "# Create tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image Classification\")\n",
    "\n",
    "# Create widgets\n",
    "browse_button = tk.Button(root, text=\"Upload Image\", command=browse_file)\n",
    "browse_button.pack(pady=10)\n",
    "\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "image_path_var = tk.StringVar()\n",
    "image_path_entry = tk.Entry(root, textvariable=image_path_var, state='readonly')\n",
    "image_path_entry.pack(pady=10)\n",
    "\n",
    "classify_button = tk.Button(root, text=\"Classify Image\", command=match_embeddings)\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "result_var = tk.StringVar()\n",
    "result_label = tk.Label(root, textvariable=result_var)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "# Function to close the tkinter window\n",
    "def close_window():\n",
    "    root.destroy()\n",
    "\n",
    "# Bind closing event to close_window function\n",
    "root.protocol(\"WM_DELETE_WINDOW\", close_window)\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
